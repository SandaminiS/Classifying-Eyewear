{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154b88ac-aec8-4481-9a57-db6ffb2f74b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b5f204-7b91-49ff-ab1b-a1c85c901b16",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86bd6c81-e26a-404a-9a72-1eb7bfea68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlassesDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = str(self.dataframe.iloc[idx, 0])  # Convert to string\n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_name}: {e}\")\n",
    "            return None, None  # Return None if there's an error loading the image\n",
    "\n",
    "        label = int(self.dataframe.iloc[idx, 1])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac6fa529-c540-48e3-92c8-40da849c5bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b35d8f0d-760a-4692-b36f-470a65423483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv('C:/Users/sanda/OneDrive - University of Central Florida/UCF/4_Spring_2024/Courses/STA6367 Statistical Methodology for Data Science II/Final Project/glasses/train.csv')\n",
    "train_directory = 'C:/Users/sanda/OneDrive - University of Central Florida/UCF/4_Spring_2024/Courses/STA6367 Statistical Methodology for Data Science II/Final Project/glasses/faces-spring-2020/faces-spring-2020/face-'\n",
    "train_df['path'] = train_df['id'].apply(lambda x: train_directory + str(x) + '.png').tolist()\n",
    "train_df = train_df[['path', 'glasses']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c2f87b-c20c-4157-897c-349658d9d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = GlassesDataset(train_df, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55be987a-b84f-4315-abac-b1dd560e6848",
   "metadata": {},
   "source": [
    "# CNN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25fe07c0-94ff-4e97-9819-d00373839ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the CNN architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 500)  # Adjusted input size for 64x64 images\n",
    "        self.fc2 = nn.Linear(500, 1)  # Output dimension is 1 for binary classification\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 32 * 16 * 16)  # Adjusted input size for 64x64 images\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))  # Sigmoid activation for binary classification\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28063359-07a0-4ff0-93bb-dfa6940de7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [100/141], Loss: 0.5888\n",
      "Epoch [2/20], Step [100/141], Loss: 0.3378\n",
      "Epoch [3/20], Step [100/141], Loss: 0.3087\n",
      "Epoch [4/20], Step [100/141], Loss: 0.2792\n",
      "Epoch [5/20], Step [100/141], Loss: 0.2578\n",
      "Epoch [6/20], Step [100/141], Loss: 0.2523\n",
      "Epoch [7/20], Step [100/141], Loss: 0.2520\n",
      "Epoch [8/20], Step [100/141], Loss: 0.2298\n",
      "Epoch [9/20], Step [100/141], Loss: 0.2136\n",
      "Epoch [10/20], Step [100/141], Loss: 0.1953\n",
      "Epoch [11/20], Step [100/141], Loss: 0.1754\n",
      "Epoch [12/20], Step [100/141], Loss: 0.1568\n",
      "Epoch [13/20], Step [100/141], Loss: 0.1398\n",
      "Epoch [14/20], Step [100/141], Loss: 0.1271\n",
      "Epoch [15/20], Step [100/141], Loss: 0.1033\n",
      "Epoch [16/20], Step [100/141], Loss: 0.0957\n",
      "Epoch [17/20], Step [100/141], Loss: 0.0913\n",
      "Epoch [18/20], Step [100/141], Loss: 0.0753\n",
      "Epoch [19/20], Step [100/141], Loss: 0.0574\n",
      "Epoch [20/20], Step [100/141], Loss: 0.0314\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model\n",
    "model = CNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training the model\n",
    "train_losses = []  # to track the training loss over epochs\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/{20}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "            train_losses.append(running_loss / 100)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "329281c6-f874-4a05-8e7c-41dd8018c3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9971111111111111\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(loader, model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            predicted = (outputs > 0.5).float()  # Predicted as positive if output > 0.5\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_accuracy = calculate_accuracy(train_loader, model)\n",
    "print(f\"Training Accuracy: {train_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d7c469-f305-49f0-8b24-fa0e7ee866c9",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73c61e0b-2537-4065-8462-d40fa9ac4b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlassesTestDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = str(self.dataframe.iloc[idx, 0])  # Convert to string\n",
    "        try:\n",
    "            image = Image.open(img_name).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7637eb23-74f2-43ae-9c63-0ba06a1fe641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df = pd.read_csv('C:/Users/sanda/OneDrive - University of Central Florida/UCF/4_Spring_2024/Courses/STA6367 Statistical Methodology for Data Science II/Final Project/glasses/test.csv')\n",
    "test_directory = 'C:/Users/sanda/OneDrive - University of Central Florida/UCF/4_Spring_2024/Courses/STA6367 Statistical Methodology for Data Science II/Final Project/glasses/faces-spring-2020/test/face-'\n",
    "test_df['path'] = test_df['id'].apply(lambda x: test_directory + str(x) + '.png').tolist()\n",
    "test_df = test_df[['path']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1979dbf3-d929-406a-b556-d190e12edcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = GlassesTestDataset(test_df, transform=transform)\n",
    "\n",
    "# Create data loader for testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        outputs = model(images)\n",
    "        predictions = torch.round(outputs)  # Round the probabilities to get binary predictions\n",
    "        test_predictions.extend(predictions.numpy().flatten())\n",
    "\n",
    "# Convert predictions to binary values\n",
    "test_predictions = [int(pred) for pred in test_predictions]\n",
    "\n",
    "# Print the predictions\n",
    "print(test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328669c-0468-44a7-b1d7-81f062b9abc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "702b4d03-8783-4557-b6b5-bfc4b1bf092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = pd.read_csv('C:/Users/sanda/OneDrive - University of Central Florida/UCF/4_Spring_2024/Courses/STA6367 Statistical Methodology for Data Science II/Final Project/glasses/ytrain.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4494bb2d-59e9-42ee-afe6-6b66fb0fc775",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = ytest.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "38c12a3f-4bb1-4fe9-9238-57b7a9445d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = ytest.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "010ad5e3-79b0-47d3-9ff7-b9d0164654c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9c015fa0-c371-4c4e-a750-9a7cb63bffed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.8\n"
     ]
    }
   ],
   "source": [
    "pred_vals = 0\n",
    "\n",
    "# Iterate over each pair of elements in test_predictions and vector\n",
    "for pred, true_val in zip(test_predictions, vector):\n",
    "    if pred == true_val:\n",
    "        pred_vals += 1\n",
    "\n",
    "print(pred_vals/len(test_predictions)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5faee3-c437-4ad2-87a7-1dfe4bfc6ea4",
   "metadata": {},
   "source": [
    "# CNN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cbf1451-37ad-401f-b548-e8e7c1972122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [100/141], Loss: 0.6537\n",
      "Epoch [2/20], Step [100/141], Loss: 0.3789\n",
      "Epoch [3/20], Step [100/141], Loss: 0.3100\n",
      "Epoch [4/20], Step [100/141], Loss: 0.2960\n",
      "Epoch [5/20], Step [100/141], Loss: 0.2880\n",
      "Epoch [6/20], Step [100/141], Loss: 0.2702\n",
      "Epoch [7/20], Step [100/141], Loss: 0.2577\n",
      "Epoch [8/20], Step [100/141], Loss: 0.2375\n",
      "Epoch [9/20], Step [100/141], Loss: 0.2204\n",
      "Epoch [10/20], Step [100/141], Loss: 0.2111\n",
      "Epoch [11/20], Step [100/141], Loss: 0.1949\n",
      "Epoch [12/20], Step [100/141], Loss: 0.1826\n",
      "Epoch [13/20], Step [100/141], Loss: 0.1719\n",
      "Epoch [14/20], Step [100/141], Loss: 0.1459\n",
      "Epoch [15/20], Step [100/141], Loss: 0.1237\n",
      "Epoch [16/20], Step [100/141], Loss: 0.1062\n",
      "Epoch [17/20], Step [100/141], Loss: 0.1011\n",
      "Epoch [18/20], Step [100/141], Loss: 0.0680\n",
      "Epoch [19/20], Step [100/141], Loss: 0.0551\n",
      "Epoch [20/20], Step [100/141], Loss: 0.0752\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, padding=2)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 500)  # Adjusted input size for 64x64 images after 4 pooling layers\n",
    "        self.fc2 = nn.Linear(500, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = CNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training the model\n",
    "train_losses = []  # to track the training loss over epochs\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.float().unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'Epoch [{epoch + 1}/{20}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 100:.4f}')\n",
    "            train_losses.append(running_loss / 100)\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ecb2a5f-b4f1-4d20-8611-27ad10d78c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = GlassesTestDataset(test_df, transform=transform)\n",
    "\n",
    "# Create data loader for testing\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in test_loader:\n",
    "        outputs = model(images)\n",
    "        predictions = torch.round(outputs)  # Round the probabilities to get binary predictions\n",
    "        test_predictions.extend(predictions.numpy().flatten())\n",
    "\n",
    "# Convert predictions to binary values\n",
    "test_predictions = [int(pred) for pred in test_predictions]\n",
    "\n",
    "# Print the predictions\n",
    "print(test_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ba1bece5-c866-413e-9651-a0d2648c0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = pd.read_csv('C:/Users/sanda/OneDrive - University of Central Florida/UCF/4_Spring_2024/Courses/STA6367 Statistical Methodology for Data Science II/Final Project/glasses/ytrain.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60d7f6fe-dea5-4590-bf2b-53267378e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = ytest.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "357ebb94-ef40-4109-a070-07fa0265c035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.8\n"
     ]
    }
   ],
   "source": [
    "pred_vals = 0\n",
    "\n",
    "# Iterate over each pair of elements in test_predictions and vector\n",
    "for pred, true_val in zip(test_predictions, vector):\n",
    "    if pred == true_val:\n",
    "        pred_vals += 1\n",
    "\n",
    "print(pred_vals/len(test_predictions)*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
